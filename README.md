# WSD
Graduation designï¼šWSD

using keras to build bilstm / bilstm+attention mechanism

the bilstm:Sequential

the added attention mechanism: Model

in addition

    the metris if f1-score : with a callbacks on epoch end
    
    the loss weight is added through the loss function 
    
    the attention mechanism : two customized layers : attention layer and concat layer
    
    other things can be learned from keras document

the base model : trainbilstm.py

the attention model : train_attention_concat.py

